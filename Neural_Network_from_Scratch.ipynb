{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b166d2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "10ace1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense_Layer:\n",
    "    def __init__(self, n_features, n_neurons):      # layer initialization\n",
    "        self.weights = np.random.randn(n_neurons, n_features).T     # transposed weight matrix\n",
    "        self.bias = np.zeros((1, n_neurons))    # bias matrix\n",
    "\n",
    "    def forward(self, input):       # defining forward pass\n",
    "        self.input = input\n",
    "        self.output = np.dot(input, self.weights) + self.bias     # wx + b\n",
    "\n",
    "    def backward(self, received_grad):      # defining backward pass\n",
    "        self.dweights = np.dot(self.input.T, received_grad)     # derivative of loss funtion w.r.t weights of the current layer = derivative of output of current layer w.r.t weights (which is input) * derivative of loss function w.r.t output of current layer (received_grad)\n",
    "        self.dbias = np.sum(received_grad, axis=0, keepdims=True)     # same as above, derivative of output of current layer w.r.t biases is just a vector of ones and its dot product with the received_grad would just sum up all its elements\n",
    "        self.dinput = np.dot(received_grad, self.weights.T)     # derivative of output of current layer w.r.t input is weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2d6152",
   "metadata": {},
   "source": [
    "Explaination of backward pass:\n",
    "\n",
    "Each row of received_grad would represent one sample. It would contain derivative of loss for that sample with respect to all the outputs of the current layer. Thus, the $j_{th}$ column represents derivative with respect to the $j_{th}$ output. $i_{th}$ row of the transposed input matrix would represent derivative of output of this layer with respect to the $i_{th}$ weight of each neuron. Their dot product thus sums over the gradient of loss of all the samples with respect to the weight of the $j_{th}$ neuron and $i_{th}$ input. The size of resulting matriz is thus same as the weights matrix.\n",
    "\n",
    "For dbias, since derivative of individual output with respect to its bias is 1, we just need to sum up gradients with respect to the output over all samples to get gradient with respect to that particular bias.\n",
    "\n",
    "For dinput, $j_{th}$ column of transposed weight matrix is derivative of output of the neuron with respect to $j_{th}$ input feature and as mentioned, $i_{th}$ row of received_grad is the derivtive of $i_{th}$ sample loss with respect to all the outputs. The dot product thus give gradient of $i_{th}$ sample loss with respect to $j_{th}$ feature. The resulting matrix thus has the same shape as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ba3b029a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        self.output = np.maximum(0, input) # the rectified linear unit activation function\n",
    "\n",
    "    def backward(self, received_grad):\n",
    "        self.dinput = received_grad.copy()\n",
    "        self.dinput[self.input <= 0] = 0   # derivative of output of ReLU w.r.t its input is 1 when the input is positive and 0 otherwise, its multiplication with received_grad would just change the non-positive values to zero and leave other values as it is.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f237408",
   "metadata": {},
   "source": [
    "Now we would implement some methods that would help update the weights and biases in order to get a model where the loss is minimized. These are known as optimizers. Below is the implementation of Stochastic Gradient Descent, with or without momentum. Momentum works as follows:\n",
    "- $\\mathbf{m} = \\beta\\mathbf{m} - \\eta\\nabla_\\theta\\mathbf{C(\\theta)}$\n",
    "- $\\theta = \\theta + \\mathbf{m}$\n",
    "\n",
    "Here, $\\theta$ is the parameter vector, $\\nabla_\\theta\\mathbf{C(\\theta)}$ is the gradient vector, $\\eta$ is the learning rate, m is the parameter momentum vector and $\\beta$, know as momentum, is a hyperparameter that shows the decay of the exponentially decaying sum of gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5f75f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    def __init__(self, learning_rate=1., decay=0., momentum=0.):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.momentum = momentum\n",
    "\n",
    "    def pre(self):\n",
    "        if self.decay:      # decay of the learning rate\n",
    "            self.current_learning_rate = self.learning_rate * (1./(1. + self.decay + self.iterations))\n",
    "\n",
    "    def update_params(self, layer):\n",
    "        if self.momentum:\n",
    "            if not hasattr(layer, 'weight_momentums'):      # if parameter momentums are not initialized\n",
    "                layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "                layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "\n",
    "            weight_update = self.momentum * layer.weight_momentums - self.current_learning_rate * layer.dweights\n",
    "            layer.weight_momentums = weight_update\n",
    "\n",
    "            bias_update = self.momentum * layer.bias_momentums - self.current_learning_rate * layer.dbias\n",
    "            layer.bias_momentums = bias_update\n",
    "\n",
    "        else:\n",
    "            weight_update = -self.current_learning_rate * layer.dweights\n",
    "            bias_update = -self.current_learning_rate * layer.dbiases\n",
    "\n",
    "        layer.weights += weight_update\n",
    "        layer.bias += bias_update\n",
    "\n",
    "    def post(self):\n",
    "        self.iterations += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d17d5f3",
   "metadata": {},
   "source": [
    "Below we create a class to implement the RMSProp (Root Mean Square Propagation) Optimizer, which works as per the following two equations:\n",
    "\n",
    "- $\\mathbf{s} = \\rho\\mathbf{s} + (1 - \\rho)\\left\\{ \\nabla_\\theta\\mathbf{C(\\theta)} \\circ \\nabla_\\theta\\mathbf{C(\\theta)} \\right\\}$\n",
    "- $\\mathbf{\\theta} = \\theta - \\eta\\left\\{ \\nabla_\\theta\\mathbf{C(\\theta)} \\oslash \\sqrt{\\mathbf{s} + \\epsilon} \\right\\}$\n",
    "\n",
    "Here, $\\circ$ denotes element-wise multiplication and $\\oslash$ denotes element-wise division, $\\rho$ denots the decay of the exponentially decaying average of squares of gradients and s (known as cache) accumulates this average. $\\epsilon$ is a small value to prevent division by zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a3d09b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSProp:\n",
    "    def __init__(self, rho=0.9, eta=0.001, epsilon=1e-7, decay=0):\n",
    "        self.learning_rate = eta\n",
    "        self.current_learning_rate = eta\n",
    "        self.rho = rho\n",
    "        self.epsilon = epsilon\n",
    "        self.iterations = 0\n",
    "        self.decay = decay\n",
    "\n",
    "    def pre(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1./(1. + self.decay * self.iterations))\n",
    "\n",
    "    def update_params(self, layer):\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.bias)\n",
    "\n",
    "        layer.weight_cache = self.rho * layer.weight_cache + (1-self.rho) * layer.dweights**2\n",
    "        layer.bias_cache = self.rho * layer.bias_cache + (1-self.rho) * layer.dbias**2\n",
    "\n",
    "        layer.weights += -self.current_learning_rate * {layer.dweights / np.sqrt(layer.weight_cache + self.epsilon)}\n",
    "        layer.bias += -self.current_learning_rate * {layer.dbias / np.sqrt(layer.bias_cache + self.epsilon)}\n",
    "\n",
    "    def post(self):\n",
    "        self.iterations += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bf3ade",
   "metadata": {},
   "source": [
    "Our final Optimizer, known as Adam, combines both the momentum method and RMSProp. It works as follows:\n",
    "\n",
    "- $\\mathbf{m} = \\beta_1\\mathbf{m} + (1 - \\beta_1)\\nabla_\\theta\\mathbf{C(\\theta)}$\n",
    "- $\\mathbf{s} = \\beta_2\\mathbf{s} + (1 - \\beta_2)\\left\\{ \\nabla_\\theta\\mathbf{C(\\theta)} \\circ \\nabla_\\theta\\mathbf{C(\\theta)} \\right\\}$\n",
    "- $\\mathbf{\\hat{m}} = \\frac{\\mathbf{m}}{1 - \\beta_1^t}$\n",
    "- $\\mathbf{\\hat{s}} = \\frac{\\mathbf{s}}{1 - \\beta_2^t}$\n",
    "- $\\theta = \\theta - \\eta\\left\\{ \\mathbf{\\hat{m}} \\oslash \\sqrt{\\mathbf{\\hat{s}} + \\epsilon} \\right\\}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ff80e35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adam:\n",
    "    def __init__(self, eta=0.001, decay=0, epsilon=1e-7, beta1=0.9, beta2=0.999):\n",
    "        self.learning_rate = eta\n",
    "        self.current_learning_rate = eta\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "\n",
    "    def pre(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "    def update_params(self, layer):\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_momentum = np.zeros_like(layer.weights)\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_momentum = np.zeros_like(layer.bias)\n",
    "            layer.bias_cache = np.zeros_like(layer.bias)\n",
    "\n",
    "        layer.weight_momentum = self.beta1 * layer.weight_momentum + (1-self.beta1) * layer.dweights\n",
    "        layer.bias_momentum = self.beta1 * layer.bias_momentum + (1-self.beta1) * layer.dbias\n",
    "\n",
    "        weight_momentum_corrected = layer.weight_momentum / (1 - self.beta1**(self.iterations + 1))\n",
    "        bias_momentum_corrected = layer.bias_momentum / (1 - self.beta1**(self.iterations + 1))\n",
    "\n",
    "        layer.weight_cache = self.beta2 * layer.weight_cache + (1-self.beta2) * layer.dweights**2\n",
    "        layer.bias_cache = self.beta2 * layer.bias_cache + (1-self.beta2) * layer.dbias**2\n",
    "\n",
    "        weight_cache_corrected = layer.weight_cache / (1 - self.beta2**(self.iterations + 1))\n",
    "        bias_cache_corrected = layer.bias_cache / (1 - self.beta2**(self.iterations + 1))\n",
    "\n",
    "        layer.weights += -self.current_learning_rate * weight_momentum_corrected / (np.sqrt(weight_cache_corrected + self.epsilon))\n",
    "        layer.bias += -self.current_learning_rate * bias_momentum_corrected / (np.sqrt(bias_cache_corrected + self.epsilon))\n",
    "\n",
    "    def post(self):\n",
    "        self.iterations += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9fa807",
   "metadata": {},
   "source": [
    "Now we implement the SoftMAX Activation funtion that is applied to the final layer of the network. It is defined as:\n",
    "$\n",
    "S_{i,j} = \\frac{e^{z_{i,j}}}{\\sum_{l=1}^{L} e^{z_{i,l}}}\n",
    "$\\\n",
    "Here, $z_{i,j}$ are the outputs of the final layer, i denotes sample number, j denotes the class number (rather, the neuron number of the final layer) and $S_{i,j}$ is the output of the activation function. Instead of $z_{i,j}$, we often use $z_{i,j} - max(z_{i,j})$ for a given sample, in order to comput the exponential.\n",
    "\n",
    "The backward method uses the following relation:\n",
    "$$\n",
    "\\frac{\\partial S_{i,j}}{\\partial z_{i,k}} = S_{i,j} \\cdot (\\delta_{j,k} - S_{i,k}) = S_{i,j} \\delta_{j,k} - S_{i,j} S_{i,k}\n",
    "$$\n",
    "\n",
    "Here, $\\delta_{j,k}$ is the Kronecker Delta function which is equal to 1 when j = k and 0 otherwise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3fb8dc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftMAX:\n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        exp_values = np.exp(input - np.max(input, axis = 1, keepdims=True))\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "        self.output = probabilities\n",
    "\n",
    "    def backward(self, received_grad):\n",
    "        self.dinputs = np.empty_like(received_grad)\n",
    "\n",
    "        for index , (single_output, single_dvalue) in enumerate(zip(self.output, received_grad)):   # we take the values of one sample at a time at evaluate the gradient for their loss.\n",
    "            single_output = single_output.reshape(-1,1)\n",
    "            jacobian_matrix = np.diagflat(single_output) - np.dot(single_output, single_output.T)   # diagflat would convert the single_output into a diagonal matrix with the single_output as the diagonal, and its multiplication with its transpose produces all the S_{i,j} * S_{i,k}.\n",
    "            self.dinputs[index] = np.dot(jacobian_matrix, single_dvalue)    # multiplying with the received gradient as per chain rule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4633d0ef",
   "metadata": {},
   "source": [
    "Explaination of jacobian matrix and chain rule used above:\n",
    "\n",
    "For a single sample, we get a 2D gradient of softmax output with respect to the input. $Element_{i,j}$ of this matrix is the gradient of $j_{th}$ output with respect to $i_{th}$ input. Thus, in the matrix multiplication of the last step, we sum up gradients with respect to the $i_{th}$ input after mulltiplying them with the respective gradients with respect to the output. This gives a scalar value that becomes the $i_{th}$ element of dinputs[index]. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0620a245",
   "metadata": {},
   "source": [
    "The following class computes loss value based on the predicted and true values. The actual loss value is: $\n",
    "L_i = -\\log(\\hat{y}_{i,k})\n",
    "$\n",
    "Here, $\\hat{y}_{i,k}$ is the predicted probablity of the sample belonging to the kth class, where k is the true class.\n",
    "\n",
    "The backward method uses the following relation to calculate the gradients: \n",
    "$$\n",
    "\\frac{\\partial L_i}{\\partial \\hat{y}_{i,j}} = -\\frac{y_{i,j}}{\\hat{y}_{i,j}}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ee2f5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Categorical_Cross_EntropyLoss:\n",
    "    def forward(self, y_pred, y_true):\n",
    "        n_samples = len(y_pred)\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1-1e-7)  # clipped so as to prevent division by zero\n",
    "\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[range(n_samples), y_true]\n",
    "\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(y_pred_clipped * y_true, axis=1)\n",
    "\n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return negative_log_likelihoods\n",
    "    \n",
    "    def backward(self, y_pred, y_true):     # since loss will be the first one to calculate a gradient during backpropagation, it wont receive any gradient (as do the other backward functions), insted it would start with the predicted values\n",
    "        n_samples = len(y_pred)\n",
    "        n_labels = len(y_pred[0])\n",
    "\n",
    "        if len(y_true.shape) == 1:      # converting to one-hot encoded if the provided one is sparse\n",
    "            y_true = np.eye(n_labels)[y_true]\n",
    "\n",
    "        self.dinputs = -y_true / y_pred\n",
    "        self.dinputs = self.dinputs / n_samples\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb90e51",
   "metadata": {},
   "source": [
    "The last phase of a typical neural network is : Final Layer -> SoftMAX Activation -> Error Calculation\\\n",
    "During backpropagation, instead of first calculating the gradient of Error with respect to SoftMAX output and then calculating gradient of SoftMAX output with respect to its input and multiplying the two as per chain rule, it is more efficient to directly calculate the gradient of Error with respect to SoftMAX input (output of the final layer). The following class implements this using the following relation:\n",
    "$$\n",
    "\\frac{\\partial L_i}{\\partial z_{i,k}} = \\hat{y}_{i,k} - y_{i,k}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6797324a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Combined_CCELoss_grad_wrt_SoftMAX_input:\n",
    "    def __init__(self):\n",
    "        self.activation = SoftMAX()\n",
    "        self.loss = Categorical_Cross_EntropyLoss()\n",
    "\n",
    "    def forward(self, input, y_true):   # input here will be the input of the softmax activation function\n",
    "        self.activation.forward(input)\n",
    "        self.output = self.activation.output\n",
    "\n",
    "        return self.loss.forward(self.output, y_true)\n",
    "    \n",
    "    def backward(self, y_pred, y_true):\n",
    "        n_samples = len(y_pred)\n",
    "\n",
    "        if len(y_true.shape) == 2:      # converting to sparse if the provided one is one-hot encoded\n",
    "            y_true = np.argmax(y_true, axis=1)      # this gives us the true classes of the samples\n",
    "\n",
    "        self.dinputs = y_pred.copy()\n",
    "        self.dinputs[range(n_samples), y_true] -= 1     # y_{i,j} will be one for all the true classes and zero otherwise\n",
    "        self.dinputs = self.dinputs / n_samples\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeca1e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# an example implementation\n",
    "\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "import matplotlib.pyplot as plt \n",
    "nnfs.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10a789e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 0, acc : 0.3433333333333333,\n",
      "epoch : 100, acc : 0.44333333333333336,\n",
      "epoch : 200, acc : 0.43666666666666665,\n",
      "epoch : 300, acc : 0.47,\n",
      "epoch : 400, acc : 0.48333333333333334,\n",
      "epoch : 500, acc : 0.4766666666666667,\n",
      "epoch : 600, acc : 0.49,\n",
      "epoch : 700, acc : 0.49,\n",
      "epoch : 800, acc : 0.5133333333333333,\n",
      "epoch : 900, acc : 0.53,\n",
      "epoch : 1000, acc : 0.5333333333333333,\n",
      "epoch : 1100, acc : 0.53,\n",
      "epoch : 1200, acc : 0.5366666666666666,\n",
      "epoch : 1300, acc : 0.55,\n",
      "epoch : 1400, acc : 0.5433333333333333,\n",
      "epoch : 1500, acc : 0.55,\n",
      "epoch : 1600, acc : 0.5666666666666667,\n",
      "epoch : 1700, acc : 0.56,\n",
      "epoch : 1800, acc : 0.57,\n",
      "epoch : 1900, acc : 0.5833333333333334,\n",
      "epoch : 2000, acc : 0.5933333333333334,\n",
      "epoch : 2100, acc : 0.5966666666666667,\n",
      "epoch : 2200, acc : 0.6166666666666667,\n",
      "epoch : 2300, acc : 0.6166666666666667,\n",
      "epoch : 2400, acc : 0.6233333333333333,\n",
      "epoch : 2500, acc : 0.6366666666666667,\n",
      "epoch : 2600, acc : 0.64,\n",
      "epoch : 2700, acc : 0.6366666666666667,\n",
      "epoch : 2800, acc : 0.6433333333333333,\n",
      "epoch : 2900, acc : 0.6433333333333333,\n",
      "epoch : 3000, acc : 0.6433333333333333,\n",
      "epoch : 3100, acc : 0.65,\n",
      "epoch : 3200, acc : 0.6533333333333333,\n",
      "epoch : 3300, acc : 0.6466666666666666,\n",
      "epoch : 3400, acc : 0.6433333333333333,\n",
      "epoch : 3500, acc : 0.6466666666666666,\n",
      "epoch : 3600, acc : 0.65,\n",
      "epoch : 3700, acc : 0.65,\n",
      "epoch : 3800, acc : 0.6533333333333333,\n",
      "epoch : 3900, acc : 0.66,\n",
      "epoch : 4000, acc : 0.6566666666666666,\n",
      "epoch : 4100, acc : 0.65,\n",
      "epoch : 4200, acc : 0.6566666666666666,\n",
      "epoch : 4300, acc : 0.6566666666666666,\n",
      "epoch : 4400, acc : 0.6633333333333333,\n",
      "epoch : 4500, acc : 0.67,\n",
      "epoch : 4600, acc : 0.6666666666666666,\n",
      "epoch : 4700, acc : 0.6633333333333333,\n",
      "epoch : 4800, acc : 0.6633333333333333,\n",
      "epoch : 4900, acc : 0.6666666666666666,\n",
      "epoch : 5000, acc : 0.67,\n",
      "epoch : 5100, acc : 0.67,\n",
      "epoch : 5200, acc : 0.6666666666666666,\n",
      "epoch : 5300, acc : 0.6833333333333333,\n",
      "epoch : 5400, acc : 0.68,\n",
      "epoch : 5500, acc : 0.69,\n",
      "epoch : 5600, acc : 0.6933333333333334,\n",
      "epoch : 5700, acc : 0.7,\n",
      "epoch : 5800, acc : 0.71,\n",
      "epoch : 5900, acc : 0.7166666666666667,\n",
      "epoch : 6000, acc : 0.72,\n",
      "epoch : 6100, acc : 0.7166666666666667,\n",
      "epoch : 6200, acc : 0.7233333333333334,\n",
      "epoch : 6300, acc : 0.73,\n",
      "epoch : 6400, acc : 0.7333333333333333,\n",
      "epoch : 6500, acc : 0.74,\n",
      "epoch : 6600, acc : 0.7533333333333333,\n",
      "epoch : 6700, acc : 0.76,\n",
      "epoch : 6800, acc : 0.7666666666666667,\n",
      "epoch : 6900, acc : 0.77,\n",
      "epoch : 7000, acc : 0.7733333333333333,\n",
      "epoch : 7100, acc : 0.77,\n",
      "epoch : 7200, acc : 0.7833333333333333,\n",
      "epoch : 7300, acc : 0.7866666666666666,\n",
      "epoch : 7400, acc : 0.79,\n",
      "epoch : 7500, acc : 0.7966666666666666,\n",
      "epoch : 7600, acc : 0.8033333333333333,\n",
      "epoch : 7700, acc : 0.8033333333333333,\n",
      "epoch : 7800, acc : 0.8066666666666666,\n",
      "epoch : 7900, acc : 0.8066666666666666,\n",
      "epoch : 8000, acc : 0.81,\n",
      "epoch : 8100, acc : 0.8133333333333334,\n",
      "epoch : 8200, acc : 0.8166666666666667,\n",
      "epoch : 8300, acc : 0.8166666666666667,\n",
      "epoch : 8400, acc : 0.8166666666666667,\n",
      "epoch : 8500, acc : 0.8133333333333334,\n",
      "epoch : 8600, acc : 0.82,\n",
      "epoch : 8700, acc : 0.82,\n",
      "epoch : 8800, acc : 0.8233333333333334,\n",
      "epoch : 8900, acc : 0.8266666666666667,\n",
      "epoch : 9000, acc : 0.8266666666666667,\n",
      "epoch : 9100, acc : 0.8233333333333334,\n",
      "epoch : 9200, acc : 0.8266666666666667,\n",
      "epoch : 9300, acc : 0.8266666666666667,\n",
      "epoch : 9400, acc : 0.8266666666666667,\n",
      "epoch : 9500, acc : 0.8266666666666667,\n",
      "epoch : 9600, acc : 0.8266666666666667,\n",
      "epoch : 9700, acc : 0.83,\n",
      "epoch : 9800, acc : 0.83,\n",
      "epoch : 9900, acc : 0.83,\n",
      "epoch : 10000, acc : 0.83,\n"
     ]
    }
   ],
   "source": [
    "x, y = spiral_data(samples=100, classes=3)\n",
    "\n",
    "dense1 = Dense_Layer(2, 64)\n",
    "activation1 = ReLU()\n",
    "dense2 = Dense_Layer(64, 3)\n",
    "activation_loss = Combined_CCELoss_grad_wrt_SoftMAX_input()\n",
    "\n",
    "optimizer = Adam()\n",
    "\n",
    "for epoch in range(10001):\n",
    "    dense1.forward(x)\n",
    "    activation1.forward(dense1.output)\n",
    "    dense2.forward(activation1.output)\n",
    "    loss = activation_loss.forward(dense2.output, y)\n",
    "\n",
    "    predictions = np.argmax(activation_loss.output, axis=1)\n",
    "    if len(y.shape) == 2:\n",
    "        y = np.argmax(y, axis=1)\n",
    "    accuracy = np.mean(predictions == y)\n",
    "\n",
    "    if not epoch % 100:\n",
    "        print(f'epoch : {epoch}, ' +\n",
    "              f'acc : {accuracy},')\n",
    "        \n",
    "    activation_loss.backward(activation_loss.output, y)\n",
    "    dense2.backward(activation_loss.dinputs)\n",
    "    activation1.backward(dense2.dinput)\n",
    "    dense1.backward(activation1.dinput)\n",
    "\n",
    "    optimizer.pre()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.post()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
